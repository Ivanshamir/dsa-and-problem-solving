# A Comprehensive Guide to Time and Space Complexity
When we solve problems with different type of algorithms, we need to analyze the computational complexity of these solutions. There are two methods for analyzing the complexity of a problem. They are: time complexity and space complexity.

## Time Complexity
From wikipedia: `In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm.` In simple word: time complexity is a measure of how much time an algorithm takes to execute as a function, based on the length of the input. It is usually expressed as a function of the input size, represented by the letter n example: O(n). And we call it order of n or Big (O) of n. But what is Big(O) and why it is important? In simple terms, if a program can able to handle large amount of input and the amount of time it takes as same as a small amount of input, then we can rate it well according to Big O Notation. Let's look at the key time and space complexities:
1. Constant: O(1)
2. Linear time: O(n)
3. Logarithmic time: O(n log n)
4. Quadratic time: O(n^2)
5. Exponential time: 2^(n)
6. Factorial time: O(n!)
### The Big-O complexity chart:
![Big O notation chart](https://thepracticaldev.s3.amazonaws.com/i/1omv0tmikzeaj24z8ps2.jpeg)
By Madison Stankevich [Dev.to](https://dev.to/madisonstankevich/big-o-notation-a-brief-overview-for-the-beginner-1o13)

It's important to note that time complexity does not measure the actual time an algorithm takes to execute, but rather how the execution time increases with the size of the input. The actual execution time depends on several factors such as the hardware, programming language, and implementation of the algorithm. Enough talking, now let's take a look at an example below and analysis the time complexity:
```
let sum = 0
for(let i = N; i > 0; i++)
    for(let j = 0; j < i; j++)
        sum += 1;
```
Now in the worst case, let's consider the scenarios:
1. N*constant operations will run for input.
2. The outer loop i will run at N times.
3. For each i, the inner j will run at N times.
So total execution time will be N*N*constant + constant. Now we will ignore the lower order terms as those are relatively insignificant for large inputs, therefore only the highest order term will be taken (without constant) which is N*N in this case.

## Space Complexity
From wikipedia: `The space complexity of an algorithm or a computer program is the amount of memory space required to solve an instance of the computational problem as a function of characteristics of the input`. In simple word: space complexity is a measure of how much memory an algorithm requires to execute as a function, based on the length of the input. It is also expressed as a function of the input size n. For example, if an algorithm takes O(n) space, it means that the amount of memory it uses grows linearly with the size of the input. For analyzing space complexity of a problem, we need to focus on two parts:
1. A fixed part: It is required for storing certain data and variables which are independent of the input size like: simple constants and variables used, instructions(code) etc.
2. A variable part: It is required for variables which are dependent on the size of the input like recursion stack, dynamic memory allocation etc. Okay now let's take a look at an example below and analysis the space complexity:
```
let sumArray = []
let sum = 0
for(let i = N; i > 0; i++)
    for(let j = 0; j < i; j++)
        sum += 1;
        sumArray.push(sum);
```
the total space required by the code can be calculated as:
Space complexity = size of sumArray + size of sum + size of i + size of j
Since sumArray has a size of N and sum i and j are all single integers, the space complexity can be simplified as:
Space complexity = N + 3
Therefore, the space complexity will be O(N).

# Data Structure Complexity Chart
<table>
  <tbody>
    <tr>
      <td>Data Structures</td>
      <td>Space Complexity</td>
      <td colspan="4">Average Case Time Complexity</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>Access</td>
      <td>Search</td>
      <td>Insertion</td>
      <td>Deletion</td>
    </tr>
    <tr>
      <td>Array</td>
      <td>O(n)</td>
      <td>O(1)</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(n)</td>
    </tr>
    <tr>
      <td>Stack</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(1)</td>
      <td>O(1)</td>
    </tr>
    <tr>
      <td>Queue</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(1)</td>
      <td>O(1)</td>
    </tr>
    <tr>
      <td>Singly Linked List</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(1)</td>
      <td>O(1)</td>
    </tr>
    <tr>
      <td>Doubly Linked List</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(1)</td>
      <td>O(1)</td>
    </tr>
    <tr>
      <td>Hash Table</td>
      <td>O(n)</td>
      <td>N/A</td>
      <td>O(1)</td>
      <td>O(1)</td>
      <td>O(1)</td>
    </tr>
    <tr>
      <td>Binary Search Tree</td>
      <td>O(n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
  </tbody>
</table>

# Search Algorithms
<table>
  <tbody>
    <tr>
      <td>Search Algorithms</td>
      <td>Space Complexity</td>
      <td colspan="3">Time Complexity</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>Best Case</td>
      <td>Average Case</td>
      <td>Worst Case</td>
    </tr>
    <tr>
      <td>Linear Search</td>
      <td>O(1)</td>
      <td>O(1)</td>
      <td>O(n)</td>
      <td>O(n)</td>
    </tr>
    <tr>
      <td>Binary Search</td>
      <td>O(1)</td>
      <td>O(1)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
  </tbody>
</table>

# Sorting Algorithms
<table>
  <tbody>
    <tr>
      <td>Sorting Algorithms</td>
      <td>Space Complexity</td>
      <td colspan="3">Time Complexity</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>Best Case</td>
      <td>Average Case</td>
      <td>Worst Case</td>
    </tr>
    <tr>
      <td>Selection Sort</td>
      <td>O(1)</td>
      <td>O(n^2)</td>
      <td>O(n^2)</td>
      <td>O(n^2)</td>
    </tr>
    <tr>
      <td>Insertion Sort</td>
      <td>O(1)</td>
      <td>O(n)</td>
      <td>O(n^2)</td>
      <td>O(n^2)</td>
    </tr>
    <tr>
      <td>Bubble Sort</td>
      <td>O(1)</td>
      <td>O(n)</td>
      <td>O(n^2)</td>
      <td>O(n^2)</td>
    </tr>
    <tr>
      <td>Quick Sort</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(n log n)</td>
      <td>O(n log n)</td>
    </tr>
    <tr>
      <td>Merge Sort</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>O(n log n)</td>
      <td>O(n log n)</td>
    </tr>
    <tr>
      <td>Heap Sort</td>
      <td>O(1)</td>
      <td>O(1)</td>
      <td>O(n log n)</td>
      <td>O(n log n)</td>
    </tr>
  </tbody>
</table>

# Complexity for Advanced Data Structures
<table>
  <tbody>
    <tr>
      <td>Data Structures</td>
      <td>Space Complexity</td>
      <td colspan="4">Average Case Time Complexity</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>Access</td>
      <td>Search</td>
      <td>Insertion</td>
      <td>Deletion</td>
    </tr>
    <tr>
      <td>Skip List</td>
      <td>O(n log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
    <tr>
      <td>Cartesian Tree</td>
      <td>O(n)</td>
      <td>N/A</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
    <tr>
      <td>B-Tree</td>
      <td>O(n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
    <tr>
      <td>Red-Black Tree</td>
      <td>O(n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
    <tr>
      <td>Splay Tree</td>
      <td>O(n)</td>
      <td>N/A</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
    <tr>
      <td>AVL Tree</td>
      <td>O(n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
    <tr>
      <td>KD Tree</td>
      <td>O(n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
      <td>O(log n)</td>
    </tr>
  </tbody>
</table>

References:
1. https://www.geeksforgeeks.org/time-complexity-and-space-complexity/
2. https://www.simplilearn.com/tutorials/data-structure-tutorial/time-and-space-complexity#GoTop
3. https://dev.to/madisonstankevich/big-o-notation-a-brief-overview-for-the-beginner-1o13
4. https://en.wikipedia.org/wiki/Time_complexity
5. https://flexiple.com/algorithms/big-o-notation-cheat-sheet/
6. https://en.wikipedia.org/wiki/Space_complexity